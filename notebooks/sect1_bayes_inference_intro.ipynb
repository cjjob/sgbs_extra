{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Chapter-1:-subjective-worlds-of-Frequentist-and-Bayesian-statistics\" data-toc-modified-id=\"Chapter-1:-subjective-worlds-of-Frequentist-and-Bayesian-statistics-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Chapter 1: subjective worlds of Frequentist and Bayesian statistics</a></span><ul class=\"toc-item\"><li><span><a href=\"#A-couple-of-useful-ideas\" data-toc-modified-id=\"A-couple-of-useful-ideas-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>A couple of useful ideas</a></span><ul class=\"toc-item\"><li><span><a href=\"#You-can-ignore-this-if-you-just-want-answers\" data-toc-modified-id=\"You-can-ignore-this-if-you-just-want-answers-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>You can ignore this if you just want answers</a></span></li></ul></li><li><span><a href=\"#The-key-takeaway\" data-toc-modified-id=\"The-key-takeaway-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>The key takeaway</a></span></li></ul></li><li><span><a href=\"#Chapter-2:-probability---the-nuts-and-bolts-of-bayesian-inference\" data-toc-modified-id=\"Chapter-2:-probability---the-nuts-and-bolts-of-bayesian-inference-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Chapter 2: probability - the nuts and bolts of bayesian inference</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: subjective worlds of Frequentist and Bayesian statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will skip the philsophical discussion here. Frequentists think in terms of repeated events and different outcomes, Bayesians are more thinking in terms of degrees of \"uncertainty\". Somewhat subtle, doesn't matter if you don't fully appreciate distinction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A couple of useful ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core idea will be Bayes' formula that allows us to move from effect to cause.\n",
    "\n",
    "So we see some evidence and then update our beliefs about what *ought to have been the case* about the underlying mechanism generating the data.\n",
    "\n",
    "Somewhat math-y:\n",
    "\n",
    "$$ \\Pr (\\text{Effect | Cause}) \\xrightarrow{\\text{Bayes' Theorem}} \\Pr (\\text{Cause | Effect}) $$\n",
    "\n",
    "And without the mysterious \"Bayes' Theorem\" stage:\n",
    "\n",
    "$$ \\Pr (C | E) = \\frac{\\Pr(E | C) \\cdot \\Pr(C)}{\\Pr (E)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can ignore this if you just want answers\n",
    "\n",
    "A bit of algebra makes this formula totally undisputable. Draw yourself a Venn diagram and observe that the numerator is $ = \\Pr (C \\cap E)$.\n",
    "\n",
    "Perhaps some more intuition can be provided by the following:\n",
    "\n",
    "$$ \\Pr (C | E) = \\Pr(C) \\cdot \\frac{\\Pr(E | C)}{\\Pr (E)}$$\n",
    "\n",
    "In this form it is clear that as the initial (soon to be called \"prior\") probability of C increases then *given any evidence/effect* the LHS term (soon \"posterior\") increases. But, more interestingly, the factor by which this increase happens is $ = \\frac{\\Pr(E | C)}{\\Pr (E)}$. This term is $ > 1$ when E is more likely if C has been observed, and is $< 1$ in the reverse case.\n",
    "\n",
    "**Inituitive example** (for this factor idea) - If I am happy almost always (e.g. 95% or $\\Pr (E = 'Happy') = 0.95$), and am always happy (so 100%, or 1) when my kill:death ratio is above 4 then if you see me and I'm happy you have some reason to believe that my kill:death ratio is above 4, but not a huge amount of reason. Conversely, if I am almost without fail miserable and you see me happy you might think actually there is quite a bit of a chance of my k:d being above 4.\n",
    "\n",
    "**Extension**\n",
    "In the example above we basically dealt with the terms *apart from* $\\Pr (C)$. If you know that I suck at online shooting games then *even if* you see me happy you still won't think my k:d > 4, because it is just so unlikely in the first instance. Instead you might *look for alternative reasons* to explain my happiness. This relates to the idea of \"explaining away\" - where the existence of one cause makes another less likely - because it is no longer required to account for the effects we observed. e.g. if you see that I am happy and you know that I just got a job promotion you become less reliant on the k:d explanation. Note, that in the limiting case where I am *only* happy when my k:d > 4, our problem reduces to propositional logic. You might therefore like to think of Bayes/stats/probability in general, as an extension of logic to the real world. In fact, you might want to question any mental distinction between math and logic you have in the first instance..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The key takeaway\n",
    "\n",
    "The last little bits are the following:\n",
    "- $prior + data \\xrightarrow{\\text{model}} posterior$\n",
    "- $p(\\theta | data) = \\frac{p(data | \\theta) p(\\theta)}{p(data)}$\n",
    "\n",
    "No explanation needed, just reformulation of the above in terms of a data generating model with parameters (a vector often), $\\mathbf{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: probability - the nuts and bolts of bayesian inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
