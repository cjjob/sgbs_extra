{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Chapter-1:-An-introduction-to-Bayesian-inference\" data-toc-modified-id=\"Chapter-1:-An-introduction-to-Bayesian-inference-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Chapter 1: An introduction to Bayesian inference</a></span></li><li><span><a href=\"#Chapter-2:-Subjective-worlds-of-Frequentist-and-Bayesian-statistics\" data-toc-modified-id=\"Chapter-2:-Subjective-worlds-of-Frequentist-and-Bayesian-statistics-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Chapter 2: Subjective worlds of Frequentist and Bayesian statistics</a></span><ul class=\"toc-item\"><li><span><a href=\"#A-couple-of-useful-ideas\" data-toc-modified-id=\"A-couple-of-useful-ideas-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>A couple of useful ideas</a></span><ul class=\"toc-item\"><li><span><a href=\"#You-can-ignore-this-if-you-just-want-answers\" data-toc-modified-id=\"You-can-ignore-this-if-you-just-want-answers-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>You can ignore this if you just want answers</a></span></li></ul></li><li><span><a href=\"#The-key-takeaway\" data-toc-modified-id=\"The-key-takeaway-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>The key takeaway</a></span></li></ul></li><li><span><a href=\"#Chapter-3:-Probability---the-nuts-and-bolts-of-bayesian-inference\" data-toc-modified-id=\"Chapter-3:-Probability---the-nuts-and-bolts-of-bayesian-inference-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Chapter 3: Probability - the nuts and bolts of bayesian inference</a></span><ul class=\"toc-item\"><li><span><a href=\"#Probability-distributions\" data-toc-modified-id=\"Probability-distributions-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Probability distributions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notation\" data-toc-modified-id=\"Notation-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Notation</a></span></li></ul></li><li><span><a href=\"#Formulae/facts\" data-toc-modified-id=\"Formulae/facts-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Formulae/facts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Independence\" data-toc-modified-id=\"Independence-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Independence</a></span></li></ul></li><li><span><a href=\"#Central-limit-theorem\" data-toc-modified-id=\"Central-limit-theorem-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Central limit theorem</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: An introduction to Bayesian inference\n",
    "\n",
    "Very very gentle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Subjective worlds of Frequentist and Bayesian statistics\n",
    "\n",
    "Will skip the philsophical discussion here. Frequentists think in terms of repeated events and different outcomes, Bayesians are more thinking in terms of degrees of \"uncertainty\". Somewhat subtle, doesn't matter if you don't fully appreciate distinction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A couple of useful ideas\n",
    "\n",
    "The core idea will be Bayes' formula that allows us to move from effect to cause.\n",
    "\n",
    "So we see some evidence and then update our beliefs about what *ought to have been the case* about the underlying mechanism generating the data.\n",
    "\n",
    "Somewhat math-y:\n",
    "\n",
    "$$ \\Pr (\\text{Effect | Cause}) \\xrightarrow{\\text{Bayes' Theorem}} \\Pr (\\text{Cause | Effect}) $$\n",
    "\n",
    "And without the mysterious \"Bayes' Theorem\" stage:\n",
    "\n",
    "$$ \\Pr (C | E) = \\frac{\\Pr(E | C) \\cdot \\Pr(C)}{\\Pr (E)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can ignore this if you just want answers\n",
    "\n",
    "A bit of algebra makes this formula totally undisputable. Draw yourself a Venn diagram and observe that the numerator is $ = \\Pr (C \\cap E)$.\n",
    "\n",
    "Perhaps some more intuition can be provided by the following:\n",
    "\n",
    "$$ \\Pr (C | E) = \\Pr(C) \\cdot \\frac{\\Pr(E | C)}{\\Pr (E)}$$\n",
    "\n",
    "In this form it is clear that as the initial (soon to be called \"prior\") probability of C increases then *given any evidence/effect* the LHS term (soon \"posterior\") increases. But, more interestingly, the factor by which this increase happens is $ = \\frac{\\Pr(E | C)}{\\Pr (E)}$. This term is $ > 1$ when E is more likely if C has been observed, and is $< 1$ in the reverse case.\n",
    "\n",
    "**Inituitive example** (for this factor idea) - If I am happy almost always (e.g. 95% or $\\Pr (E = \\text{Happy}) = 0.95$), and am always happy (so 100%, or 1) when my kill:death ratio is above 4 then if you see me and I'm happy you have some reason to believe that my kill:death ratio is above 4, but not a huge amount of reason. Conversely, if I am almost without fail miserable and you see me happy you might think actually there is quite a bit of a chance of my k:d being above 4.\n",
    "\n",
    "**Extension**\n",
    "In the example above we basically dealt with the terms *apart from* $\\Pr (C)$. If you know that I suck at online shooting games then *even if* you see me happy you still won't think my k:d > 4, because it is just so unlikely in the first instance. Instead you might *look for alternative reasons* to explain my happiness. This relates to the idea of \"explaining away\" - where the existence of one cause makes another less likely - because it is no longer required to account for the effects we observed. e.g. if you see that I am happy and you know that I just got a job promotion you become less reliant on the k:d explanation. Note, that in the limiting case where I am *only* happy when my k:d > 4, our problem reduces to propositional logic. You might therefore like to think of Bayes/stats/probability in general, as an extension of logic to the real world. In fact, you might want to question any mental distinction between math and logic you have in the first instance..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The key takeaway\n",
    "\n",
    "The last little bits are the following:\n",
    "- $prior + data \\xrightarrow{\\text{model}} posterior$\n",
    "- $p(\\theta | data) = \\frac{p(data | \\theta) p(\\theta)}{p(data)}$\n",
    "\n",
    "No explanation needed, just reformulation of the above in terms of a data generating model with parameters (a vector often), $\\mathbf{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Probability - the nuts and bolts of bayesian inference\n",
    "\n",
    "Just a quick summary of a few facts and properties that will be relied on extensively. Not much explanation will be added here. (Anyone with a decent maths background should find no dragons here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability distributions\n",
    "\n",
    "These will be the bread and butter of all that we do. In particular, because, unlike a point estimate (e.g. the mean of a distribution) they implicitly 'carry along' the uncertainty (and where it is placed) associated with our belief. Remember, uncertainty is what Bayesian analysis is all about.\n",
    "\n",
    "Distributions are:\n",
    "- non-negative for any event\n",
    "- sum to 1 across the entire possibility space\n",
    "\n",
    "For discrete cases we speak of \"probability mass\". For continuous cases we speak in terms of \"densities\". For any specific value the probability density is zero.\n",
    "\n",
    "Note, we do not commit the common logical fallacy!\n",
    "\n",
    "Impossible $\\rightarrow p(x) = 0$.\n",
    "\n",
    "But...\n",
    "\n",
    "$p(x) = 0 \\nrightarrow$ impossible.\n",
    "\n",
    "i.e. our events all having zero *density* is not us saying that they will never occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notation\n",
    "\n",
    "- Discrete: $\\Pr (X = a)$\n",
    "- Continuous: $p(X = \\theta)$\n",
    "\n",
    "However, you will often see real abuses of notation occur with continuous cases.\n",
    "\n",
    "e.g. $p_{random\\_variable}(RV = \\alpha)$, becomes...\n",
    "\n",
    "$p_A(\\alpha)$, becomes...\n",
    "\n",
    "$p(A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulae/facts\n",
    "\n",
    "Will skip the discrete versions, they're all analogous.\n",
    "\n",
    "- Sum to 1:\n",
    "    - $\\int p(x) dx = 1$\n",
    "- Expected value:    \n",
    "    - $\\mu = \\mathbb{E}[x] = \\int x \\cdot p(x) dx$\n",
    "- Marginal distribution:\n",
    "    - $p_A(\\alpha) = \\int p_{AB}(\\alpha, \\beta) d\\beta$\n",
    "    - OR $p(A) = \\int p(A, B) dB$\n",
    "- Conditional distribution:\n",
    "    - $p (A | B) = \\frac{p(A, B)}{p(B)}$\n",
    "- Independent:\n",
    "    - $p(A | B) = p(A)$\n",
    "    - $p(A, B) = p(A) \\cdot P(B)$\n",
    "        - It is very easy to verify that the above two expressions are imply and are implied by each other.\n",
    "        - Note as well that to show independence, one must show that this relationship holds for any possible value of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independence\n",
    "\n",
    "Note that, not being independent, (i.e. \"dependence\"), is not the kind of dependence that might first spring to mind. It is not any kind of causal comment, rather simply that information about one gives some information about the other. The reason could be direct (e.g. a parent's hair colour to a child's), co-varying (e.g. sibling intelligence), or even a complicated process that is not obvious at all (note this still falls into the \"co-varying\" scenario, just the degree of separation can be massive is the point being made here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central limit theorem\n",
    "\n",
    "Without this guy, stats would be in some trouble...!\n",
    "\n",
    "CLT is applicable generally when sample size > 20. It says the distribution of an RV becomes approximately normal.\n",
    "\n",
    "> The above CLT applies to the average of independent, identically distributed random variables. However, there are also central limit theorems that apply far less stringent conditions. This means that whenever an output is the result of the sum or average of a number of largely independent factors, then it may be reasonable to assume it is normally distributed. For example, one can argue that an individual's intelligence is the result of the average of a number of factors, including parenting, genetics, life experience and health, among others. Hence, we might assume that an individual's test score picked at random from the population is normally distributed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
